{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of EBSNN\n",
    "\n",
    "1. Temporarily implemented a fast version, just to make training possible\n",
    "    1. features, labels (for train, valid, and test set)\n",
    "    2. pickle for file (no need of h5py with enough memory)\n",
    "    3. segmentation is not done here\n",
    "2. Then implement detailed preprocessing\n",
    "    1. handle errors\n",
    "    2. removing features like IP addresses as described in the paper\n",
    "3. I want the preprocessing to handle details and nothing special to do in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weibo total 16980 packets\n",
      "failed files: []\n",
      "class kugou total 2427 packets\n",
      "failed files: []\n",
      "class cloudmusic total 55548 packets\n",
      "failed files: []\n",
      "class pplive total 2856 packets\n",
      "failed files: []\n",
      "class itunes total 8666 packets\n",
      "failed files: []\n",
      "class facebook total 0 packets\n",
      "failed files: ['facebook_video2b.pcap']\n",
      "class spotify total 3145 packets\n",
      "failed files: []\n",
      "class tudou total 37437 packets\n",
      "failed files: []\n",
      "class youtube total 7689 packets\n",
      "failed files: []\n",
      "class skype total 0 packets\n",
      "failed files: ['unprocessed_skype_video1b.pcap', 'extra_skype_audio3.pcap']\n",
      "class sohu total 13976 packets\n",
      "failed files: []\n",
      "class voipbuster total 1485 packets\n",
      "failed files: []\n",
      "class MS-Exchange total 289 packets\n",
      "failed files: []\n",
      "class aimchat total 1139 packets\n",
      "failed files: []\n",
      "class vimeo total 2991 packets\n",
      "failed files: []\n",
      "class yahoomail total 1700 packets\n",
      "failed files: []\n",
      "class gmail total 0 packets\n",
      "failed files: ['extra_gmailchat2.pcap', 'extra_gmailchat3.pcap', 'gmailchat1.pcap']\n",
      "class netflix total 51885 packets\n",
      "failed files: []\n",
      "class thunder total 84744 packets\n",
      "failed files: []\n",
      "class amazon total 2170 packets\n",
      "failed files: []\n",
      "class qq total 166 packets\n",
      "failed files: []\n",
      "class sinauc total 84 packets\n",
      "failed files: []\n",
      "class google total 10664 packets\n",
      "failed files: []\n",
      "class taobao total 23329 packets\n",
      "failed files: []\n",
      "class twitter total 14636 packets\n",
      "failed files: []\n",
      "class youku total 25655 packets\n",
      "failed files: []\n",
      "class mssql total 110 packets\n",
      "failed files: []\n",
      "class baidu total 6422 packets\n",
      "failed files: []\n",
      "class jd total 12786 packets\n",
      "failed files: []\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import dpkt\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# def calculate_alpha(counter, mode='normal'):\n",
    "#     if mode == 'normal':\n",
    "#         alpha = torch.tensor(counter, dtype=torch.float32)\n",
    "#         alpha = alpha / alpha.sum(0).expand_as(alpha)\n",
    "#     elif mode == 'invert':\n",
    "#         alpha = torch.tensor(counter, dtype=torch.float32)\n",
    "#         alpha_sum = alpha.sum(0)\n",
    "#         alpha_sum_expand = alpha_sum.expand_as(alpha)\n",
    "#         alpha = (alpha_sum - alpha) / alpha_sum_expand\n",
    "#     # fill all zeros to ones\n",
    "#     alpha[alpha==0.] = 1.\n",
    "#     return alpha\n",
    "\n",
    "\n",
    "def process_buffer(buffer, max_length=1500):\n",
    "    \"\"\"\n",
    "    TODO: detailed processing of packet data (read the paper)\n",
    "\n",
    "    DPKT docs: https://kbandla.github.io/dpkt/\n",
    "    \"\"\"\n",
    "    try:\n",
    "        eth = dpkt.ethernet.Ethernet(buffer)\n",
    "        if not isinstance(eth.data, dpkt.ip.IP):\n",
    "            return None\n",
    "        ip = eth.data\n",
    "        if not isinstance(ip.data, dpkt.tcp.TCP):\n",
    "            return None\n",
    "        tcp = ip.data\n",
    "        payload = tcp.data\n",
    "    except Exception as e:\n",
    "        print(\"[error] {}\".format(e))\n",
    "\n",
    "    # redundant if do padding here\n",
    "    return bytes(ip)   # debug\n",
    "\n",
    "\n",
    "def read_class(class_name, data_dir):\n",
    "    \"read a class of packets\"\n",
    "    features = []\n",
    "    count = 0\n",
    "    failed_files = []\n",
    "    for file in os.listdir(os.path.join(data_dir, class_name)):\n",
    "        with open(f'../data/d1/{class_name}/{file}', 'rb') as f:\n",
    "            try:\n",
    "                pcap = dpkt.pcap.Reader(f)\n",
    "            except Exception as e:\n",
    "                failed_files.append(file)\n",
    "                continue\n",
    "            \n",
    "            for timestamp, buffer in pcap:\n",
    "                processed_data = process_buffer(buffer)\n",
    "                if processed_data is not None:  # TODO: better handling\n",
    "                    features.append(processed_data)\n",
    "                    count += 1\n",
    "        break   # FIXME: data size not consistent with paper (weibo 80k vs. 50k), break just for debugging\n",
    "    \n",
    "    print(f\"class {class_name} total {count} packets\")   # NOTE by zian: does flow needs extra processing ?\n",
    "    print(\"failed files:\", failed_files)\n",
    "    return features\n",
    "\n",
    "\n",
    "def read_dataset(data_dir):\n",
    "    \"dataset `d1` or `d2`\"\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    label2id = {}\n",
    "    id2label = {}\n",
    "\n",
    "    for i, class_name in enumerate(os.listdir(data_dir)):\n",
    "        label2id[class_name] = i\n",
    "        id2label[i] = class_name\n",
    "        class_features = read_class(class_name, data_dir)\n",
    "        class_labels = [i for j in range(len(class_features))]\n",
    "        features += class_features\n",
    "        labels += class_labels\n",
    "    \n",
    "    return features, labels, label2id, id2label\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    X, y, label2id, id2label = read_dataset('../data/d1')\n",
    "    print(y[:5])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "    with open('../data/d1_train_dump.pkl', 'wb') as f:\n",
    "        pickle.dump(X_train, f)\n",
    "        pickle.dump(y_train, f)\n",
    "        pickle.dump(label2id, f)\n",
    "        pickle.dump(id2label, f)\n",
    "    \n",
    "    with open('../data/d1_val_dump.pkl', 'wb') as f:\n",
    "        pickle.dump(X_val, f)\n",
    "        pickle.dump(y_val, f)\n",
    "        pickle.dump(label2id, f)\n",
    "        pickle.dump(id2label, f)\n",
    "    \n",
    "    with open('../data/d1_test_dump.pkl', 'wb') as f:\n",
    "        pickle.dump(X_test, f)\n",
    "        pickle.dump(y_test, f)\n",
    "        pickle.dump(label2id, f)\n",
    "        pickle.dump(id2label, f)\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('torchenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4aaf13baed617d3d49cfcda231e454f3fa42d009d9c9f3ef7549cb249f54be8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
